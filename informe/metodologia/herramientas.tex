En esta sección presentamos las herramientas y los comandos con los que se realizó el preentrenamiento, el entrenamiento y la generación de oraciones para la tesis.

\subsection{Festival y Festvox: Generación de transcripciones fonéticas}

Festival \cite{festivalDownload} es un framework que permite sintetizar habla. Además posee una gran variedad de APIs para el procesamiento de audios y generación de nuevos sistemas TTS. Festvox \cite{festvoxDownload} a su vez expande sobre Festival, agregando todavía más herramientas relacionadas a la síntesis y generación de modelos, que van desde la generación de modelos prosódicos, hasta etiquetado automático de corpus.

Para este trabajo utilizamos Festival y Festvox para generar las oraciones requeridas tanto para el entrenamiento como para la síntesis de audios. Estas transcripciones consisten en una lista de fonos dividida en segmentos temporales y datos contextuales tales como la cantidad de sílabas en la palabra siendo transcrita, fonos que preceden y proceden al actual, etc. A modo de guía, a continuación mostraremos cómo utilizamos estas herramientas para generar las transcripciones fonéticas deseadas usando EHMM alignment.

Primero tenemos que generar un archivo \textit{txt.done.data} donde estén los nombres de cada archivo de audio y su transcripción grafémica. Por ejemplo, en el siguiente recuadro podemos ver un extracto del archivo generado para SECYT\_mm utilizado para este proceso:

\begin{tcolorbox}
( SECYT\_mm\_1\_335 ``Algunos dicen gamba en vez de pierna'' )

( SECYT\_mm\_1\_29 ``El conjunto de las escenas se reitera en el galpón'' )

( SECYT\_mm\_1\_361 ``Lluvia con truenos en Medellín'' )

( SECYT\_mm\_1\_619 ``Rendían pleistecía vikingo conquistador'' )

( SECYT\_mm\_1\_110 ``Llueve sobre las piedras de la pared'' )

( SECYT\_mm\_1\_102 ``Las etapas del desarrollo infantil difieren según el niño'' )

\end{tcolorbox}

En este trabajo utilizamos Festival 2.4 \cite{festivalDownload}, Festvox 2.7 \cite{festvoxDownload} y speech\_tools 2.4 \cite{speechToolDownload} para la generación de transcripciones fonéticas. Para poder utilizarlos agregamos las siguientes variables de entorno en nuestro PATH:

\begin{tcolorbox}
export PATH=/project/festival/bin:\$PATH

export PATH=/project/speech\_tools/bin:\$PATH

export FESTVOXDIR=/project/festvox

export ESTDIR=/project/speech\_tools
\end{tcolorbox}

Luego generamos los directorios, scripts y archivos necesarios para generar una nueva voz:
%referencia
%https://github.com/zeehio/festvox/blob/master/src/clustergen/setup_cg
\begin{tcolorbox}
\$FESTVOXDIR/src/clustergen/setup\_cg uba es SECYT\_mm
\end{tcolorbox}

En la nueva estructura de archivos generada, copiamos los audios en la carpeta wav/ y el archivo \textit{txt.done.data} previamente generado en la carpeta etc/.

Además en los archivos 

\begin{tcolorbox}
festvox/uba\_es\_\_cg.scm 

festvox/uba\_es\_\_clunits.scm
\end{tcolorbox}

\noindent es necesario cambiar las dependencias 

\begin{tcolorbox}
(require 'uba\_es\_\_phoneset)

(require 'uba\_es\_\_lexicon)
\end{tcolorbox}

\noindent que contienen los símbolos fonéticos del español de España, por estas otras:

\begin{tcolorbox}
(require 'uba\_es\_\_phoneset\_mex)

(require 'uba\_es\_\_lexicon\_mex)
\end{tcolorbox}

\noindent que contienen el conjunto de símbolos fonéticos del español mexicano, que se aproximan mucho a los del castellano rioplatense (no contiene [th], por ejemplo).

Finalmente, corriendo los siguientes comandos:

\begin{tcolorbox}
./bin/do\_build build\_prompts

./bin/do\_build label

./bin/do\_build build\_utts
\end{tcolorbox}

\noindent se realizará el proceso de alineamiento y transcripción automática. Una vez finalizado se habrán generado las transcripciones fonéticas con formato .utt en el directorio festival/utts, que entre otra metadata tiene codificados los fonos de la oración, sus principios y sus finales.

De manera análoga, esta herramienta permite crear partituras utilizadas para la síntesis. Simplemente generando un archivo \textit{txt.done.data} con oraciones que se quieran sintetizar, y corriendo el script

\begin{tcolorbox}
./bin/do\_build build\_prompts ./synth/txt.done.data
\end{tcolorbox}

En la carpeta utt gen/prompt-utt se habrán generado los .utt necesarios para la síntesis.


\subsection{HTS}


HTS \cite{hts} es un framework de entrenamiento y síntesis de sistemas TTS basado en HMMs que modela simultáneamente la duración, el espectro (mel-cepstrum) y la frecuencia principal ($f0$) utilizando una combinación de HMMs.

\begin{figure}
\includegraphics[scale=0.5]{imagenes/hmm.png}
\caption{Estructura de un HMM (tomado de \cite{phoneticAndProsodic}, página 41)}
\label{hmmStructure}
\centering
\end{figure}

La Figura \ref{hmmStructure} resume la estructura de un HMM usado para síntesis del habla. El espectro y la frecuencia fundamental son modelados en paralelo usando vectores separados. En particular el espectro es modelado como un vector de Gaussianas $n$ dimensional, mientras que la frecuencia principal es modelada como un conjunto de vectores de Gaussianas de dimensión uno y cero.

Al mismo tiempo HTS toma la decisión de modelar la información prosódica dentro de este mismo framework. Para esto, las distribuciones para el espectro, la frecuencia principal y las duraciones son clusterizadas independientemente utilizando la información contextual extraída de los audios de entrenamiento. A modo ilustrativo en la Figura \ref{hmmTree} se muestra una esquematización de un HMM resultante, utilizando árboles de decisión para clusterizar los datos. Notar que cada hoja del árbol resultante coincide con un vector $n$ dimensional de Gaussianas o un conjunto de vector de Gaussianas de dimensión cero y uno, según corresponda al espectro o a la frecuencia principal.

\begin{figure}
\includegraphics[scale=0.5]{imagenes/hmmContext.png}
\caption{Esquema HMM generado utilizando árboles de decisión (tomado de \cite{phoneticAndProsodic}, página 45)}
\label{hmmTree}
\centering
\end{figure}
%hablar de los distintos tipos de clusters?

Si bien existen muchas maneras de clusterizar el conjunto de fonos, que pueden variar desde algoritmos simples hasta técnicas que utilicen redes neuronales, para este trabajo todos los entrenamientos y clusterizaciones de datos se realizaron con árboles de decisión. 

Por otro lado, como información contextual para el entrenamiento se tomaron los dos fonos por anteriores y posteriores a cada fono y la siguiente información fonética.

\begin{itemize}
\item Modo de articulación del fono:
\begin{itemize}
\item Oclusivo
\item Líquido
\item Fricativo
\item Africativo
\item Nasal
\end{itemize}
\item Punto de articulación del fono:
\begin{itemize}
\item Palatal
\item Labial
\item Alveolar
\item Labio-dental
\item Dental
\item Velar
\end{itemize}
\item La perspectiva articulatoria (anterior, central o posterior).
\item Si el fono es una vocal o una consonante.
\item En caso de ser una vocal, a que categoría pertenece: por ejemplo para el fono $[i]:${$i$ (no acentuada), $i0$ (diptongo), $i1$ (acentuada)}.
\item En caso de ser una vocal, su redondeamiento vocálico.
\item En caso de ser una consonante, si es débil o fuerte.
\end{itemize}

De esta manera HTS espera tener una voz más dinámica, que para diferentes valores contextuales darán diferentes modelos acústicos para cada fono.

En la Figura \ref{genTree} se muestra el resultado de un fragmento de uno de los árboles de decisión generado para modelar la duración de un fono. En base a este modelo, el sistema podrá inferir, por ejemplo, que si el fono actual no es nasal (C-Nasal) seguido de un stop (R-Stop), que no es el fono $l$ estará modelado por función de probabilidad Gaussiana definida en $dur\_s2\_7$.

\begin{figure}
\begin{center}
\includegraphics[scale=0.4]{imagenes/arbolDeDesicionTesis.png}
\caption{Árbol de decisión generado a partir de los datos para la duración de un HMM}
\label{genTree}
\end{center}
\end{figure}

En las primeras iteraciones del desarrollo no contábamos con la información acústica, por lo que se generaron modelos carentes de información contextual. En estos primeros modelos se pudo apreciar una calidad muy inferior en los audios generados, sonando estos sumamente metálicos y carentes de prosodia. Esto se debía, posiblemente, a que los árboles de decisión no tenían información contextual suficiente para ser construidos de manera efectiva, resultando en una mala generalización y malos audios sintetizados. Tras agregar los factores contextuales y realizar algunas pruebas de concepto, pudimos comprobar que las voces sonaban con mucha mejor calidad.


\subsection{Entrenamiento} \label{entrenamientoHTS}

Desde un punto de vista puramente técnico, utilizar HTS para entrenar un modelo es bastante sencillo. Asumiendo que todos los paquetes necesarios fueron instalados, es posible entrenar una nueva voz adaptando los scripts disponibles en la página de descargas de HTS.

Para ello, reemplazamos los audios en la carpeta data/raw con aquellos que se quieran utilizar y los utts correspondientes previamente generados. Además, como adelantamos en la sección anterior, le indicamos a HTS qué información contextual se utilizará para la clusterización, por lo que es necesario modificar el archivo data/questions/questions\_qst001.hed con la información contextual apropiada para una voz en castellano. En el Apéndice \ref{apendiceQuestions} se presenta el archivo utilizado para Loc1\_pal.

Una vez finalizadas estas modificaciones, en la carpeta data/ de la demo puede iniciarse el pre-entrenamiento de la siguiente manera:

\begin{tcolorbox}
make
\end{tcolorbox}

\noindent Esto extraerá features acústicos del audio y construirá los archivos de entrenamiento, entre otras cosas. Finalmente para dar comienzo al entrenamiento, ejecutamos en la carpeta raíz:

\begin{tcolorbox}
perl scripts/Training.pl scripts/Config.pm $>$ train.log 2$>$ err.log
\end{tcolorbox}

Una vez que se completa el entrenamiento, podemos encontrar en la carpeta voices/qst001/ver1 el modelo generado (.htsvoice).

Para este trabajo todos los audios usaron sampling rate de $48$kHz, precisión de $16$bits, mono. Además HTS requiere que explicitemos un rango de extracción para la frecuencia fundamental de la voz. Tanto para \textit{SECYT-mujer} como para \textit{Loc1-Pal} el rango utilizado utilizado fue desde $100$hz hasta $350$hz, mientras que para \textit{CMU-ARCTIC} el rango de extracción fue desde $110$kHz hasta $280$kHz.

Estos parámetros y muchos otros pueden ser configurados fácilmente corriendo

\begin{tcolorbox}
./configure
\end{tcolorbox}

\noindent en la carpeta raíz del proyecto. En la próxima sección detallaremos como utilizar varios .htsvoice generados para mezclar y sintetizar una nueva voz.

\subsection{HTS\_engine} \label{interpolationTeory}

Finalmente, para generar voces con acento extranjero se utilizó hts\_engine. Esta herramienta, entre otras cosas, permite interpolar entre varios modelos, para producir un nuevo modelo con una mezcla de la carga fonética y prosódica. Esto nos brinda un gran rango exploratorio y nos permite ajustar la carga fonética de los modelos entrenados previamente para cumplir con nuestro objetivo de sintetizar habla en castellano con acento inglés.

A grandes rasgos, la interpolación consiste en tomar los vectores generados anteriormente durante el entrenamiento e interpolar sus funciones de densidad Gaussianas para obtener una nueva. En la Figura \ref{spekerInterpolationImagen} (extraída del trabajo \cite{SpekerInterpolationRef}) puede verse la interpolación de $N$ HMMs, cada uno con peso arbitrario $a_1$, $a_2$, ..., $a_N$, que generan un nuevo modelo $\Lambda$.

\begin{figure}
\begin{center}
\includegraphics[scale=0.4]{imagenes/speakerInterpolation.png}
\caption{Block diagram of speech synthesis system with speaker interpolation. (tomado de \cite{phoneticAndProsodic}, página $70$) }
\label{spekerInterpolationImagen}
\end{center}
\end{figure}

Una vez obtenido el nuevo HMM, el proceso de síntesis puede ocurrir como para cualquier otro modelo, ilustrado en la etapa de síntesis de la figura. El procedimiento para sintetizar una nueva oración es bastante simple. Asumiendo que todas las dependencias fueron instaladas de manera correcta, con el siguiente comando es posible utilizar los modelos generados en \textit{cmu\_us\_arctic\_slt.htsvoice} y \textit{models/loc1\_pal.htsvoice} para interpolar con peso $0.7$ y $0.3$ respectivamente, generar un nuevo modelo y sintetizar la oración presente en el archivo in.lab.

\begin{tcolorbox}
hts\_engine -m models/cmu\_us\_arctic\_slt.htsvoice -m models/loc1\_pal.htsvoice -i 2 0.7 0.3 -ow out.wav in.lab
\end{tcolorbox}

Esta herramienta permite además modificar el pitch, la duración del audio y otros aspectos de la síntesis. La documentación completa puede encontrarse en la pagina web: http://hts-engine.sourceforge.net/.