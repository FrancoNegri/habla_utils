En esta sección presentamos las herramientas utilizadas para este trabajo. Así también como los comandos con los que se realizó en preentrenamiento, el entrenamiento y la generación de oraciones para la tesis.

\subsection{Festival y Festvox: Generación las transcripciones foneticas}

Festival es un framework que permite sintetizar habla. Además posee una gran variedad de APIs, para el procesamiento de audios y generación de nuevos sistemas TTS. Festvox a su vez expande sobre Festival, agregando todavía más herramientas relacionadas a la síntesis y generación de modelos, que van desde la generación de modelos prosódicos, hasta etiquetado automático de corpus.

Para este trabajo utilizaremos Festival y Festvox para generar los oraciones requeridos tanto para el entrenamiento como para la síntesis de audios. Estos consisten básicamente en una transcripción fonética de los audios dividida en segmentos temporales y datos contextuales tales como la cantidad de silabas en la palabra siendo transcrita, fonemas que preceden y proceden al actual, etc.

A modo de guía a continuación mostraremos como es que utlizamos estas herramientas para generar las transcripciones foneticas deseadas usando EHMM alignment.

Primero tendremos que generar un archivo \textit{txt.done.data} donde esten los nombres de cada archivo de audio y su transcripcion grafemica. Por ejemplo, en el siguiente recuadro podemos ver un extracto del archivo generado para SECYT\_mm utilizado para este proceso:

\begin{tcolorbox}
( SECYT\_mm\_1\_335 ``Algunos dicen gamba en vez de pierna'' )

( SECYT\_mm\_1\_29 ``El conjunto de las escenas se reitera en el galpón'' )

( SECYT\_mm\_1\_361 ``Lluvia con truenos en Medellín'' )

( SECYT\_mm\_1\_619 ``Rendían pleistecía vikingo conquistador'' )

( SECYT\_mm\_1\_110 ``Llueve sobre las piedras de la pared'' )

( SECYT\_mm\_1\_102 ``Las etapas del desarrollo infantil difieren según el niño'' )

\end{tcolorbox}

En este trabajo utilizamos Festival $2.4$\cite{festivalDownload}, Festvox $2.7$\cite{festvoxDownload} y speech\_tools $2.4$\cite{speechToolDownload} para la generación de transcripciones foneticas. Para poder utilizarlos agregamos las siguientes variables de entorno en nuestro PATH:

\begin{tcolorbox}
export PATH=/project/festival/bin:\$PATH

export PATH=/project/speech\_tools/bin:\$PATH

export FESTVOXDIR=/project/festvox

export ESTDIR=/project/speech\_tools
\end{tcolorbox}

Luego generamos los directorios, scripts y archivos necesarios para generar una nueva voz:
%referencia
%https://github.com/zeehio/festvox/blob/master/src/clustergen/setup_cg
\begin{tcolorbox}
\$FESTVOXDIR/src/clustergen/setup\_cg uba es SECYT\_mm
\end{tcolorbox}

En la nueva estructura de archivos generada, copiar los audios en la carpeta wav/ y el archivo \textit{txt.done.data} previamente generado en la carpeta etc/.

Ademas en los archivos 

\begin{tcolorbox}
festvox/uba\_es\_\_cg.scm 

festvox/uba\_es\_\_clunits.scm
\end{tcolorbox}

Es necesario cambiar las dependencias 

\begin{tcolorbox}
(require 'uba\_es\_\_phoneset)

(require 'uba\_es\_\_lexicon)
\end{tcolorbox}

que contienen los simbolos foneticos del español de españa, por estas otras

\begin{tcolorbox}
(require 'uba\_es\_\_phoneset\_mex)

(require 'uba\_es\_\_lexicon\_mex)
\end{tcolorbox}

que contienen el conjunto de simbolos foneticos del español mexicano que se aproximan de manera muy cercana a los del castellano rio platense (no contiene /th/ por ejemplo).

Finalmente corriendo los siguientes comandos:

\begin{tcolorbox}
./bin/do\_build build\_prompts

./bin/do\_build label

./bin/do\_build build\_utts
\end{tcolorbox}

Se realizará el proceso de alineamiento y transcripción automatica. Una vez finalizado se habran generado las transcripciones foneticas con formato .utt en el directorio festival/utts, que entre otra metadata tiene codificados los fonos de la oración, sus principios y sus finales.

De manera analoga, esta herramienta te permite crear transcripciones foneticas para la sintesis. Simplemente generando un archivo \textit{txt.done.data} con oracines que se quieran sintetizar, y corriendo el script

\begin{tcolorbox}
./bin/do\_build build\_prompts ./synth/txt.done.data
\end{tcolorbox}

En la carpeta utt gen/prompt-utt se habrán generado los .utt necesarios para la sintesis.


\subsection{HTS}

\faltaContarComoPasarDeUTTAOtroFormato

HTS es un framework de entrenamiento y sintesis de sistemas TTS basado en HMMs que modela simultáneamente la duración, el espectro (mel-cepstrum) y la frecuencia principal ($f0$) de utilizando una combinación de HMMs:

\begin{figure}
\includegraphics[scale=0.5]{imagenes/hmm.png}
\caption{Estructura de un hmm (simultaneous modeling of phonetic and prosodic parameters, and characteristic conversion for hmm-based text-to-speech systems, takayoshi yoshimura, january 2002, pag. 41)}
\label{hmmStructure}
\centering
\end{figure}

Como se ve en la figura \ref{hmmStructure} el espectro y la frecuencia fundamental son modelados en paralelo usando vectores separados. En particual el espectro será modelado como un vector de gaussianas $n$ dimencional, mientras que la frecuencia principal será modelado como conjunto de vectores de gaussianas de dimención uno y cero.

Al mismo tiempo HTS toma la decisión de modelar la información prosódica dentro de este mismo framework. Para esto, las distribuciones para el espectro, la frecuencia principal y las duraciones son clusterizadas independientemente utilizando la información contextual extraída de los audios de entrenamiento. A modo ilustrativo en la figura \ref{hmmTree} se muestra una esquematización de un HMM resultante utilizando arboles de decición para clusterizar los datos. Notar que cada hoja del arbol resultante coincidirá con un vector $n$ dimencional de gaussianas o un conjunto de vectores de gaussianas de dimención cero y uno, segun corresponda al espectro o a la frecuencia principal.

\begin{figure}
\includegraphics[scale=0.5]{imagenes/hmmContext.png}
\caption{Esquema HMM generado utilizando arboles de decisión (simultaneous modeling of phonetic and prosodic parameters, and characteristic conversion for hmm-based text-to-speech systems, takayoshi yoshimura, january 2002, pag. 45)}
\label{hmmTree}
\centering
\end{figure}
%hablar de los distintos tipos de clusters?

Si bien existen muchas maneras de clusterizar el conjunto de fonemas, que pueden variar desde algoritmos simples hasta tecnicas de redes neuronales, para este trabajo todos los entrenamientos y clusterizaciones de datos se realizarán con arboles de decisión. 

Por otro lado como información contextual para el entrenamiento se tomaron los dos fonemas precedentes y los dos fonemas procedentes para cada fonema y la siguiente información fonetica.

\begin{itemize}
\item Modo de articulación del fonema.
\item Punto de articulación del fonema.
\item La perspectiva articulatoria (anterior, central o posterior).
\item Si el fonema es una vocal o una consonante.
\item En caso de ser una vocal, a que categoría pertenece: por ejemplo para el fonema $/i/:$ {$i$ (no acentuada), $i0$ (diptongo) ,$i1$ (acentuada)}.
\item En caso de ser una vocal, su redondeamiento vocálico.
\item En caso de ser una consonante, si es lennis o fortis.
\end{itemize}

De esta manera HTS espera tener una voz mas dinamica, que para diferentes valores contextuales darán diferentes modelos acusticos para cada fonema.

En la imagen \ref{genTree} se muestra el resultado de un fragmento de uno de los arboles de decisión generado para modelar la duración de un fonema. En base a este modelo, el sistema podrá inferir por ejemplo que si el fonema actual no es nasal (C-Nasal) seguido de un stop (R-Stop), que no es el fonema $l$ estará modelado por función de probabilidad gaussiana definida en $dur\_s2\_7$.

\begin{figure}
\begin{center}
\includegraphics[scale=0.4]{imagenes/arbolDeDesicionTesis.png}
\caption{Árbol de decisión generado a partir de los datos para la duración de un HMM}
\label{genTree}
\end{center}
\end{figure}

En las primeras iteraciones del desarrollo no contábamos con la información acústica por lo que se generaron modelos carentes de información contextual. En estos primeros modelos se pudo apreciar una calidad mucho peor en los audios generados, sonando estos sumamente metálicos y carentes de prosodia. Esto se debía, posiblemente, a que los arboles de decición no tenían información contextual suficiente para ser construidos de manera efectiva, resultando en una mala generalización y malos audios sintetizados. Tras agregar los factores contextuales y realizar algunas pruebas de concepto con ellas pudimos comprobar que las voces sonaban mucho mas humanas.


\subsection{Entrenamiento} \label{entrenamientoHTS}

Para este trabajo todos los audios usarán sampling rate de $48$kHz, precisión de $16$bits, mono. 

Ademas HTS nos pide que expicitemos un rango de extracción para frecuencia fundamental. Tanto para SECYT como para Loc1-Pal el rango utilizado fue desde $100$hz hasta $350$hz, mientras que para CMU-ARCTIC el rango de extracción fue desde $110$kHz hasta $280$kHz.

Una lista extensiva de los parámetros utilizados para el entrenamiento se puede ver en el apéndice $3$.

Detalles tecnicos...
%HTS también brinda la posibilidad de realizar speaker-adaptive training. Esta técnica permite tomar un modelo ya entrenado y adaptarlo para asimilar características de un nuevo hablante. Esta técnica nace de la idea que construir un corpus de datos es costoso tanto en espacio de almacenamiento, tiempo de grabación y etiquetado, por lo que resulta mas económico generar una nueva voz sintética a partir de un modelo bien generado y adaptándolo luego con características del nuevo corpus.
\detallesTecnicos

%Dentro del adaptative training existen varias técnicas, en este trabajo utilizaremos offline supervised adaptation, que tiene como requisito adicional conocer los oraciones del segundo corpus. 

% Cosas para hablar:
% contextual factors: cuales son, para que sirven.
% arboles de decición.
% questions
% expandir sobre hmms

\subsection{HTS\_engine} \label{interpolationTeory}

Finalmente para generar voces con acento extranjero se utilizó hts\_engine. Esta herramienta permite interpolar con pesos arbitrarios entre varios modelos para producir un nuevo modelo con una mezcla de la carga fonética y prosódica de ambos hablantes y sintetizar audios. Esto nos brinda un gran rango exploratorio y nos permitirá ajustar la carga fonética de los modelos originales para acercarnos al modelo deseado. 

A grandes rasgos la interpolación consistirá en tomar los vectores generados anteriormente durante el entrenamiento e interpolar sus funciones de densidad gaussianas para obtener una nueva. En la imagen \ref{spekerInterpolationImagen} (extraida del trabajo \cite{SpekerInterpolationRef}) puede verse la interpolación de $N$ HMMs, cada uno con peso arbitrario $a_1$, $a_2$, ..., $a_N$, que generaran un nuevo modelo $\Lambda$.

\begin{figure}
\begin{center}
\includegraphics[scale=0.4]{imagenes/speakerInterpolation.png}
\caption{Block diagram of speech synthesis system with speaker interpolaiton. }
\label{spekerInterpolationImagen}
\end{center}
\end{figure}

Una vez obtenido el nuevo HMM es generado el proceso de sintesis puede ocurrir como para cualquier otro modelo, ilustrado en la etapa de sintesis de la figura.

Detalles tecnicos...

\detalllesTecnicos